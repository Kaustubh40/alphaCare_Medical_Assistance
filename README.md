# alphaCare_Medical_Assistance
**How to Run the Project (simple demo flow)**

1. Open Google Colab→ https://colab.research.google.com

2. Upload your notebooks:

colab_finetune.ipynb (training, if they want to see the process)

inference_demo.ipynb (testing, the main one you'll show for outputs)

3. Change runtime:

Go to Runtime Change runtime type

Select:

Python 3.10

GPU (T4/A100, anything available)

Save.

4. Run the notebook cells one by one:

First cell installs dependencies (transformers, peft, trl, etc.)

The model and tokenizer load from Hugging Face (microsoft/phi-2).

Your fine-tuned LoRA adapter folder (alpacare_lora_adapter) is loaded.

5. Test outputs (in inference_demo.ipynb):
How to Run the Project (simple demo flow)

1. Open Google Colab→ https://colab.research.google.com

2. Upload your notebooks:

colab_finetune.ipynb (training, if they want to see the process)

inference_demo.ipynb (testing, the main one you'll show for outputs)

3. Change runtime:

Go to Runtime Change runtime type

Select:

Python 3.10

GPU (T4/A100, anything available)

Save.

4. Run the notebook cells one by one:

First cell installs dependencies (transformers, peft, trl, etc.)

The model and tokenizer load from Hugging Face (microsoft/phi-2).

Your fine-tuned LoRA adapter folder (alpacare_lora_adapter) is loaded.

5. Test outputs (in inference_demo.ipynb):
   How to Run the Project (simple demo flow)

1. Open Google Colab→ https://colab.research.google.com

2. Upload your notebooks:

colab_finetune.ipynb (training, if they want to see the process)

inference_demo.ipynb (testing, the main one you'll show for outputs)

3. Change runtime:

Go to Runtime Change runtime type

Select:

Python 3.10

GPU (T4/A100, anything available)

Save.

4. Run the notebook cells one by one:

First cell installs dependencies (transformers, peft, trl, etc.)

The model and tokenizer load from Hugging Face (microsoft/phi-2).

Your fine-tuned LoRA adapter folder (alpacare_lora_adapter) is loaded.

5. Test outputs (in inference_demo.ipynb):

# -*- coding: utf-8 -*-
"""colab-finetune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14_zM5Yg6ezuF--PrukMY6gNsgbMCk0wj
"""

!pip install -q transformers==4.44.2 peft==0.13.0 accelerate==0.34.2 datasets==3.0.1 bitsandbytes==0.43.1 trl==0.8.1 sentencepiece==0.2.0

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
import torch

dataset = load_dataset("lavita/AlpaCare-MedInstruct-52k")
dataset = dataset["train"].train_test_split(test_size=0.1, seed=42)
train_data = dataset["train"]
test_data = dataset["test"]

print(train_data[0])

base_model = "microsoft/phi-2"
tokenizer = AutoTokenizer.from_pretrained(base_model)
model = AutoModelForCausalLM.from_pretrained(
    base_model,
    torch_dtype=torch.float16,
    device_map=None
).to("cuda")

tokenizer.pad_token = tokenizer.eos_token

def add_disclaimer(example):
    text = (
        f"### Instruction:\n{example['input']}\n\n"
        f"### Response:\n{example['output']}\n\n"
        "Disclaimer: This is educational only — consult a qualified clinician."
    )
    return {"text": text}

train_data = train_data.map(add_disclaimer)
test_data = test_data.map(add_disclaimer)

def tokenize_fn(batch):
    return tokenizer(batch["text"], truncation=True, padding="max_length", max_length=512)

train_tokenized = train_data.map(tokenize_fn, batched=True)
test_tokenized = test_data.map(tokenize_fn, batched=True)

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)
model = get_peft_model(model, lora_config)

training_args = TrainingArguments(
    output_dir="./alpacare_lora",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    num_train_epochs=1,
    logging_steps=10,
    save_strategy="epoch",
    fp16=True,
    report_to="none",
)

from trl import SFTTrainer
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./alpacare_lora",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    num_train_epochs=1,
    logging_steps=10,
    save_strategy="epoch",
    fp16=False,                # ✅ disable fp16 mode
    bf16=False,                # ✅ disable bf16 too
    report_to="none",
)

trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=train_tokenized,
    eval_dataset=test_tokenized,
    dataset_text_field="text",
    tokenizer=tokenizer,
    max_seq_length=512,
)

trainer.train()

model.save_pretrained("./alpacare_lora_adapter")
tokenizer.save_pretrained("./alpacare_lora_adapter")